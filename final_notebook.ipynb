{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40004ebe",
   "metadata": {},
   "source": [
    "# LOG Per Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f21f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langkah 1: Loading Data dan Eksplorasi Awal...\n",
      "Data Training Shape: (3313, 4)\n",
      "Data Test Shape: (1787, 3)\n",
      "\n",
      "Info Data Training:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3313 entries, 0 to 3312\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3313 non-null   int64 \n",
      " 1   title     3313 non-null   object\n",
      " 2   abstract  3313 non-null   object\n",
      " 3   labels    3313 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 103.7+ KB\n",
      "\n",
      "Missing Values di Data Training:\n",
      "id          0\n",
      "title       0\n",
      "abstract    0\n",
      "labels      0\n",
      "dtype: int64\n",
      "\n",
      "Missing Values di Data Test:\n",
      "id          0\n",
      "title       0\n",
      "abstract    0\n",
      "dtype: int64\n",
      "\n",
      "Semua Label SDG yang Teridentifikasi (16):\n",
      "['sdg_1', 'sdg_10', 'sdg_11', 'sdg_12', 'sdg_13', 'sdg_14', 'sdg_15', 'sdg_16', 'sdg_2', 'sdg_3', 'sdg_4', 'sdg_5', 'sdg_6', 'sdg_7', 'sdg_8', 'sdg_9']\n",
      "\n",
      "Frekuensi Setiap Label SDG:\n",
      "sdg_1: 105\n",
      "sdg_10: 170\n",
      "sdg_11: 74\n",
      "sdg_12: 187\n",
      "sdg_13: 77\n",
      "sdg_14: 208\n",
      "sdg_15: 88\n",
      "sdg_16: 133\n",
      "sdg_2: 190\n",
      "sdg_3: 2072\n",
      "sdg_4: 113\n",
      "sdg_5: 138\n",
      "sdg_6: 182\n",
      "sdg_7: 215\n",
      "sdg_8: 246\n",
      "sdg_9: 215\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Untuk text cleaning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Untuk multi-label encoding\n",
    "from sklearn.multioutput import MultiOutputClassifier # Untuk Binary Relevance strategy\n",
    "from sklearn.linear_model import LogisticRegression # Model klasifikasi biner\n",
    "from sklearn.metrics import f1_score # Metrik evaluasi\n",
    "import joblib # Untuk menyimpan model dan vectorizer\n",
    "\n",
    "# --- Tambahkan impor dan kode ini di sini ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2 # Ukuran validation set\n",
    "MAX_FEATURES = 10000 # Batasan jumlah fitur TF-IDF\n",
    "NGRAM_RANGE = (1, 2) # Menggunakan unigram dan bigram\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "# --- Langkah 1: Data Loading dan Initial Explorasi ---\n",
    "print(\"Langkah 1: Loading Data dan Eksplorasi Awal...\")\n",
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "\n",
    "print(\"Data Training Shape:\", train_df.shape)\n",
    "print(\"Data Test Shape:\", test_df.shape)\n",
    "\n",
    "print(\"\\nInfo Data Training:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nMissing Values di Data Training:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values di Data Test:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Analisis Target Variabel (labels)\n",
    "# Mendapatkan daftar semua unique labels yang mungkin\n",
    "all_labels = sorted(list(set([lbl for labels in train_df['labels'] for lbl in labels.split()])))\n",
    "print(f\"\\nSemua Label SDG yang Teridentifikasi ({len(all_labels)}):\")\n",
    "print(all_labels)\n",
    "\n",
    "# Hitung frekuensi setiap label (untuk memahami imbalance)\n",
    "label_counts = {}\n",
    "for labels in train_df['labels']:\n",
    "    for label in labels.split():\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(\"\\nFrekuensi Setiap Label SDG:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "# Catatan: Macro F1 sangat dipengaruhi oleh performa pada label minor.\n",
    "# Frekuensi rendah menunjukkan label tersebut akan sulit diprediksi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b0978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 2: Preprocessing Data...\n",
      "Menerapkan Text Cleaning...\n",
      "Menerapkan Multi-label Binarization...\n",
      "Urutan Label setelah Binarization: ['sdg_1', 'sdg_10', 'sdg_11', 'sdg_12', 'sdg_13', 'sdg_14', 'sdg_15', 'sdg_16', 'sdg_2', 'sdg_3', 'sdg_4', 'sdg_5', 'sdg_6', 'sdg_7', 'sdg_8', 'sdg_9']\n"
     ]
    }
   ],
   "source": [
    "# --- Langkah 2: Preprocessing Data ---\n",
    "print(\"\\nLangkah 2: Preprocessing Data...\")\n",
    "\n",
    "# Gabungkan title dan abstract\n",
    "train_df['text'] = train_df['title'].fillna('') + ' ' + train_df['abstract'].fillna('')\n",
    "test_df['text'] = test_df['title'].fillna('') + ' ' + test_df['abstract'].fillna('')\n",
    "\n",
    "# Fungsi Text Cleaning\n",
    "def clean_text(text): # Gunakan nama fungsi yang sudah ada di notebook Anda\n",
    "    text = str(text).lower() # Pastikan input adalah string\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Hapus non-alfabet dan non-spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Hapus spasi ganda dan trim\n",
    "\n",
    "    # --- Tambahkan baris ini untuk menghapus stop words ---\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOP_WORDS]\n",
    "    text = \" \".join(words)\n",
    "    # --- Akhir penambahan ---\n",
    "\n",
    "    return text\n",
    "\n",
    "print(\"Menerapkan Text Cleaning...\")\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Processing Label (Multi-label Binarization)\n",
    "print(\"Menerapkan Multi-label Binarization...\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Fit Binarizer pada semua label yang ada di data train\n",
    "y_train_bin = mlb.fit_transform(train_df['labels'].apply(lambda x: x.split()))\n",
    "\n",
    "# Label order setelah binarisasi\n",
    "print(\"Urutan Label setelah Binarization:\", list(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae48c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 3: Feature Engineering (TF-IDF)...\n",
      "Shape Data Training setelah TF-IDF: (3313, 15000)\n",
      "Shape Data Test setelah TF-IDF: (1787, 15000)\n"
     ]
    }
   ],
   "source": [
    "# --- Langkah 3: Feature Engineering (TF-IDF Vectorization) ---\n",
    "print(\"\\nLangkah 3: Feature Engineering (TF-IDF)...\")\n",
    "\n",
    "# Inisialisasi TF-IDF Vectorizer\n",
    "# Fit vectorizer hanya pada data training yang sudah bersih\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000, # Coba tingkatkan jumlah fitur\n",
    "    ngram_range=(1, 3), # Gunakan Unigram, Bigram, dan Trigram\n",
    "    min_df=3, # Coba turunkan minimal dokumen yang mengandung term\n",
    ")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text']) # Transform data test menggunakan vectorizer yang sudah di-fit\n",
    "\n",
    "print(\"Shape Data Training setelah TF-IDF:\", X_train_tfidf.shape)\n",
    "print(\"Shape Data Test setelah TF-IDF:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a745035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    aa  aas   ab  abdomen  abdominal  abdominal ct  abdominal pain  \\\n",
      "0  0.0  0.0  0.0      0.0        0.0           0.0             0.0   \n",
      "1  0.0  0.0  0.0      0.0        0.0           0.0             0.0   \n",
      "2  0.0  0.0  0.0      0.0        0.0           0.0             0.0   \n",
      "3  0.0  0.0  0.0      0.0        0.0           0.0             0.0   \n",
      "4  0.0  0.0  0.0      0.0        0.0           0.0             0.0   \n",
      "\n",
      "   abelmoschus  abelmoschus esculentus  abi  ...   zo  zone  zones  zoom  \\\n",
      "0          0.0                     0.0  0.0  ...  0.0   0.0    0.0   0.0   \n",
      "1          0.0                     0.0  0.0  ...  0.0   0.0    0.0   0.0   \n",
      "2          0.0                     0.0  0.0  ...  0.0   0.0    0.0   0.0   \n",
      "3          0.0                     0.0  0.0  ...  0.0   0.0    0.0   0.0   \n",
      "4          0.0                     0.0  0.0  ...  0.0   0.0    0.0   0.0   \n",
      "\n",
      "   zoonotic  zoothamnium   zs  zs rights  zs rights reserved  zscore  \n",
      "0       0.0          0.0  0.0        0.0                 0.0     0.0  \n",
      "1       0.0          0.0  0.0        0.0                 0.0     0.0  \n",
      "2       0.0          0.0  0.0        0.0                 0.0     0.0  \n",
      "3       0.0          0.0  0.0        0.0                 0.0     0.0  \n",
      "4       0.0          0.0  0.0        0.0                 0.0     0.0  \n",
      "\n",
      "[5 rows x 15000 columns]\n"
     ]
    }
   ],
   "source": [
    "# contoh output\n",
    "import pandas as pd\n",
    "\n",
    "# Konversi sparse matrix ke dense matrix lalu ke DataFrame\n",
    "X_train_df = pd.DataFrame(\n",
    "    X_train_tfidf.toarray(), \n",
    "    columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Tampilkan beberapa baris pertama\n",
    "print(X_train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19fa422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 4 & 5: Pelatihan Model, Validasi & Tuning Threshold...\n",
      "Shape Data Training Part (-19.0%): (2650, 15000)\n",
      "Shape Data Validation (20.0%): (663, 15000)\n",
      "Memulai Pelatihan Model Binary Relevance...\n",
      "Pelatihan Selesai.\n",
      "Mulai Tuning Threshold...\n",
      "Threshold Optimal yang ditemukan di Validation: 0.0890\n",
      "Macro F1 di Validation dengan Threshold Optimal: 0.5579\n",
      "Macro F1 di Validation (konfirmasi): 0.5579\n"
     ]
    }
   ],
   "source": [
    "# --- Langkah 4 & 5: Pemilihan & Pelatihan Model, Validasi & Tuning ---\n",
    "print(\"\\nLangkah 4 & 5: Pelatihan Model, Validasi & Tuning Threshold...\")\n",
    "\n",
    "# Strategy: Binary Relevance (satu model biner per label)\n",
    "# Model dasar: Logistic Regression\n",
    "# Gunakan solver 'liblinear' yang baik untuk data sparse\n",
    "base_model = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE)\n",
    "multioutput_classifier = MultiOutputClassifier(base_model, n_jobs=-1) # n_jobs=-1: gunakan semua core CPU\n",
    "\n",
    "# Split data training untuk validasi\n",
    "X_train_part, X_val, y_train_part, y_val = train_test_split(\n",
    "    X_train_tfidf, y_train_bin, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    # Untuk stratifikasi multi-label, butuh library tambahan atau implementasi custom.\n",
    "    # Untuk demo ini, kita gunakan split standar dulu.\n",
    ")\n",
    "\n",
    "print(f\"Shape Data Training Part ({1-TEST_SIZE*100}%):\", X_train_part.shape)\n",
    "print(f\"Shape Data Validation ({TEST_SIZE*100}%):\", X_val.shape)\n",
    "\n",
    "print(\"Memulai Pelatihan Model Binary Relevance...\")\n",
    "multioutput_classifier.fit(X_train_part, y_train_part)\n",
    "print(\"Pelatihan Selesai.\")\n",
    "\n",
    "# Evaluasi pada Validation Set (Menggunakan Macro F1)\n",
    "# Model Logistic Regression dengan MultiOutputClassifier akan memberikan probabilitas\n",
    "y_val_proba = np.transpose([model.predict_proba(X_val)[:, 1] for model in multioutput_classifier.estimators_])\n",
    "\n",
    "\n",
    "# Tuning Threshold\n",
    "# Cari threshold terbaik untuk memaksimalkan Macro F1 di validation set\n",
    "best_threshold = 0.5 # Default\n",
    "best_macro_f1 = 0\n",
    "\n",
    "print(\"Mulai Tuning Threshold...\")\n",
    "# Coba range threshold dari 0.1 hingga 0.9 dengan langkah 0.05\n",
    "thresholds = np.arange(0.01, 1.0, 0.001)\n",
    "for threshold in thresholds:\n",
    "    # Konversi probabilitas menjadi prediksi biner menggunakan threshold saat ini\n",
    "    y_val_pred_tuned = (y_val_proba > threshold).astype(int)\n",
    "\n",
    "    # Hitung Macro F1 untuk threshold ini\n",
    "    macro_f1 = f1_score(y_val, y_val_pred_tuned, average='macro')\n",
    "\n",
    "    # Update threshold terbaik jika Macro F1 lebih baik\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Threshold Optimal yang ditemukan di Validation: {best_threshold:.4f}\")\n",
    "print(f\"Macro F1 di Validation dengan Threshold Optimal: {best_macro_f1:.4f}\")\n",
    "\n",
    "# Dengan threshold optimal, buat prediksi biner untuk validation set\n",
    "y_val_pred_optimal = (y_val_proba > best_threshold).astype(int)\n",
    "print(f\"Macro F1 di Validation (konfirmasi): {f1_score(y_val, y_val_pred_optimal, average='macro'):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6a695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 5: Per-Label Threshold Tuning...\n",
      "Mulai Per-Label Threshold Tuning (Mencoba 990 threshold per label)...\n",
      "\n",
      "Menghitung Macro F1 Total dengan Threshold Optimal Per-Label di Validation...\n",
      "\n",
      "--- Hasil Setelah Per-Label Threshold Tuning ---\n",
      "Threshold Optimal Per-Label:\n",
      "  sdg_1: 0.0660\n",
      "  sdg_10: 0.1040\n",
      "  sdg_11: 0.0350\n",
      "  sdg_12: 0.1500\n",
      "  sdg_13: 0.0690\n",
      "  sdg_14: 0.1220\n",
      "  sdg_15: 0.1040\n",
      "  sdg_16: 0.0890\n",
      "  sdg_2: 0.0620\n",
      "  sdg_3: 0.5130\n",
      "  sdg_4: 0.0600\n",
      "  sdg_5: 0.1000\n",
      "  sdg_6: 0.1260\n",
      "  sdg_7: 0.1650\n",
      "  sdg_8: 0.1380\n",
      "  sdg_9: 0.0930\n",
      "\n",
      "Macro F1 di Validation dengan Threshold Optimal Per-Label: 0.6443\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------BARUUUU------------------------------------\n",
    "# Cell 5: Per-Label Threshold Tuning\n",
    "\n",
    "print(\"Cell 5: Per-Label Threshold Tuning...\")\n",
    "\n",
    "# Gunakan probabilitas prediksi dari model LGBM di validation set (dari Cell 4)\n",
    "# Variabel y_val_proba dan y_val seharusnya sudah tersedia dari Cell 4\n",
    "# Variabel thresholds (range threshold) juga harus sudah tersedia dari Cell 4\n",
    "# Variabel mlb (MultiLabelBinarizer) juga harus sudah tersedia dari Cell 2\n",
    "# Variabel label_classes (urutan nama label) juga harus sudah tersedia dari Cell 2\n",
    "\n",
    "current_y_val_proba = y_val_proba # Probabilitas prediksi di validation set dari model di Cell 4\n",
    "current_y_val = y_val # Ground truth biner validation set dari Cell 4\n",
    "\n",
    "# Simpan threshold optimal untuk setiap label\n",
    "best_thresholds_per_label = []\n",
    "macro_f1_per_label_individual_list = [] # Opsional, simpan F1 individual\n",
    "\n",
    "print(f\"Mulai Per-Label Threshold Tuning (Mencoba {len(thresholds)} threshold per label)...\")\n",
    "# Pastikan label_classes (urutan nama label) sudah tersedia dari Cell 2\n",
    "label_classes = list(mlb.classes_) # Ambil lagi untuk memastikan\n",
    "\n",
    "# Loop melalui setiap label (kolom di y_val_proba)\n",
    "for label_idx in range(current_y_val_proba.shape[1]):\n",
    "    # print(f\"  Tuning threshold untuk label: {label_classes[label_idx]}...\") # Opsional, untuk melihat progress\n",
    "    best_f1_for_label = -1 # F1 bisa 0, mulai dari -1\n",
    "    optimal_threshold_for_label = 0.5 # Default awal\n",
    "\n",
    "    # Dapatkan probabilitas dan target ground truth hanya untuk label ini\n",
    "    label_proba = current_y_val_proba[:, label_idx]\n",
    "    label_true = current_y_val[:, label_idx]\n",
    "\n",
    "    # Coba berbagai threshold dari range 'thresholds' yang didefinisikan di Cell 4\n",
    "    for threshold in thresholds:\n",
    "        label_pred_tuned = (label_proba > threshold).astype(int)\n",
    "\n",
    "        # Hitung F1 score untuk label ini\n",
    "        # zero_division=1 penting jika label_true tidak memiliki positive sample di validation\n",
    "        # (misal, label yang sangat jarang tidak muncul di split validation)\n",
    "        f1_for_label = f1_score(label_true, label_pred_tuned, zero_division=1)\n",
    "\n",
    "        # Update threshold terbaik untuk label ini\n",
    "        if f1_for_label > best_f1_for_label:\n",
    "            best_f1_for_label = f1_for_label\n",
    "            optimal_threshold_for_label = threshold\n",
    "\n",
    "    # Simpan threshold optimal dan F1 score terbaik untuk label ini\n",
    "    best_thresholds_per_label.append(optimal_threshold_for_label)\n",
    "    macro_f1_per_label_individual_list.append(best_f1_for_label)\n",
    "\n",
    "\n",
    "# Setelah tuning untuk semua label, hitung Macro F1 total dengan threshold optimal per label\n",
    "print(\"\\nMenghitung Macro F1 Total dengan Threshold Optimal Per-Label di Validation...\")\n",
    "# Terapkan array threshold ke array probabilitas\n",
    "y_val_pred_optimal_per_label = (current_y_val_proba > np.array(best_thresholds_per_label)).astype(int) # FIX: Gunakan current_y_val_proba\n",
    "macro_f1_optimal_per_label = f1_score(current_y_val, y_val_pred_optimal_per_label, average='macro', zero_division=1)\n",
    "\n",
    "print(f\"\\n--- Hasil Setelah Per-Label Threshold Tuning ---\")\n",
    "print(f\"Threshold Optimal Per-Label:\")\n",
    "for i, thresh in enumerate(best_thresholds_per_label):\n",
    "     print(f\"  {label_classes[i]}: {thresh:.4f}\")\n",
    "\n",
    "print(f\"\\nMacro F1 di Validation dengan Threshold Optimal Per-Label: {macro_f1_optimal_per_label:.4f}\")\n",
    "# Bandingkan dengan hasil global threshold LGBM dari Cell 4\n",
    "# Anda perlu mengingat skor global threshold dari output Cell 4, atau menyimpannya ke variabel di Cell 4\n",
    "# print(\"Perbandingan dengan Hasil Sebelumnya (Global Threshold):\", macro_f1_optimal_per_label - best_macro_f1_global)\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "# Variabel best_thresholds_per_label ini SANGAT PENTING. Ini akan digunakan untuk prediksi akhir pada data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 6: Prediksi pada Test Data dan Pembuatan Submission File...\n",
      "Submission file 'submission.csv' berhasil dibuat.\n",
      "\n",
      "--- Langkah Lanjutan ---\n",
      "Pendekatan yang lebih powerful biasanya melibatkan Fine-tuning Model Transformer.\n",
      "Contoh:\n",
      "1. Gunakan library Hugging Face Transformers.\n",
      "2. Pilih model pre-trained yang sesuai, misal 'allenai/scibert_scivocab_uncased' untuk domain ilmiah.\n",
      "3. Siapkan Dataset dan DataLoader untuk multi-label classification.\n",
      "4. Fine-tune model dengan layer klasifikasi multi-label (16 output + Sigmoid) di atasnya.\n",
      "5. Latih model dengan Binary Cross-Entropy loss.\n",
      "6. Validasi dan tuning threshold tetap KRUSIAL seperti di atas.\n",
      "7. Pertimbangkan ensemble model Transformer (misal, rata-rata probabilitas dari beberapa run atau model yang berbeda).\n",
      "Implementasi ini lebih kompleks dan membutuhkan GPU.\n"
     ]
    }
   ],
   "source": [
    "# --- Langkah 6: Prediksi dan Submission ---\n",
    "print(\"\\nLangkah 6: Prediksi pada Test Data dan Pembuatan Submission File...\")\n",
    "\n",
    "# Dapatkan probabilitas prediksi pada data test penuh\n",
    "y_test_proba = np.transpose([model.predict_proba(X_test_tfidf)[:, 1] for model in multioutput_classifier.estimators_])\n",
    "\n",
    "# Terapkan threshold optimal yang ditemukan dari validation set\n",
    "y_test_pred_bin = (y_test_proba > np.array(best_thresholds_per_label)).astype(int)\n",
    "\n",
    "# Konversi prediksi biner kembali ke format string label\n",
    "# Gunakan mlb.inverse_transform untuk mengubah array biner menjadi daftar label string\n",
    "test_pred_labels = mlb.inverse_transform(y_test_pred_bin)\n",
    "\n",
    "# Format untuk submission: gabungkan label per baris menjadi satu string\n",
    "submission_labels = [\" \".join(labels) for labels in test_pred_labels]\n",
    "\n",
    "# Buat DataFrame Submission\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'labels': submission_labels})\n",
    "\n",
    "# Simpan Submission File\n",
    "submission_df.to_csv('submission_log.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' berhasil dibuat.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcaa0b",
   "metadata": {},
   "source": [
    "# lgbm per label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7182c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Untuk text cleaning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Untuk multi-label encoding\n",
    "from sklearn.multioutput import MultiOutputClassifier # Untuk Binary Relevance strategy\n",
    "from sklearn.linear_model import LogisticRegression # Model klasifikasi biner\n",
    "from sklearn.metrics import f1_score # Metrik evaluasi\n",
    "import joblib # Untuk menyimpan model dan vectorizer\n",
    "\n",
    "# --- Tambahkan impor dan kode ini di sini ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import lightgbm as lgb # Akan digunakan nanti, import saja di awal\n",
    "\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2 # Ukuran validation set\n",
    "MAX_FEATURES = 10000 # Batasan jumlah fitur TF-IDF\n",
    "NGRAM_RANGE = (1, 2) # Menggunakan unigram dan bigram\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "# --- Langkah 1: Data Loading dan Initial Explorasi ---\n",
    "print(\"Langkah 1: Loading Data dan Eksplorasi Awal...\")\n",
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "\n",
    "print(\"Data Training Shape:\", train_df.shape)\n",
    "print(\"Data Test Shape:\", test_df.shape)\n",
    "\n",
    "print(\"\\nInfo Data Training:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nMissing Values di Data Training:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values di Data Test:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Analisis Target Variabel (labels)\n",
    "# Mendapatkan daftar semua unique labels yang mungkin\n",
    "all_labels = sorted(list(set([lbl for labels in train_df['labels'] for lbl in labels.split()])))\n",
    "print(f\"\\nSemua Label SDG yang Teridentifikasi ({len(all_labels)}):\")\n",
    "print(all_labels)\n",
    "\n",
    "# Hitung frekuensi setiap label (untuk memahami imbalance)\n",
    "label_counts = {}\n",
    "for labels in train_df['labels']:\n",
    "    for label in labels.split():\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(\"\\nFrekuensi Setiap Label SDG:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "# Catatan: Macro F1 sangat dipengaruhi oleh performa pada label minor.\n",
    "# Frekuensi rendah menunjukkan label tersebut akan sulit diprediksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Langkah 2: Preprocessing Data ---\n",
    "print(\"\\nLangkah 2: Preprocessing Data...\")\n",
    "\n",
    "# Gabungkan title dan abstract\n",
    "train_df['text'] = train_df['title'].fillna('') + ' ' + train_df['abstract'].fillna('')\n",
    "test_df['text'] = test_df['title'].fillna('') + ' ' + test_df['abstract'].fillna('')\n",
    "\n",
    "# Fungsi Text Cleaning\n",
    "def clean_text(text): # Gunakan nama fungsi yang sudah ada di notebook Anda\n",
    "    text = str(text).lower() # Pastikan input adalah string\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Hapus non-alfabet dan non-spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Hapus spasi ganda dan trim\n",
    "\n",
    "    # --- Tambahkan baris ini untuk menghapus stop words ---\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOP_WORDS]\n",
    "    text = \" \".join(words)\n",
    "    # --- Akhir penambahan ---\n",
    "\n",
    "    return text\n",
    "\n",
    "print(\"Menerapkan Text Cleaning...\")\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Processing Label (Multi-label Binarization)\n",
    "print(\"Menerapkan Multi-label Binarization...\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Fit Binarizer pada semua label yang ada di data train\n",
    "y_train_bin = mlb.fit_transform(train_df['labels'].apply(lambda x: x.split()))\n",
    "\n",
    "# --- Tambahkan baris ini di sini: ---Baru-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "label_classes = list(mlb.classes_) # Menyimpan urutan nama label\n",
    "\n",
    "# Label order setelah binarisasi\n",
    "print(\"Urutan Label setelah Binarization:\", list(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d797253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Langkah 3: Feature Engineering (TF-IDF Vectorization) ---\n",
    "print(\"\\nLangkah 3: Feature Engineering (TF-IDF)...\")\n",
    "\n",
    "# Inisialisasi TF-IDF Vectorizer\n",
    "# Fit vectorizer hanya pada data training yang sudah bersih\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000, # Coba tingkatkan jumlah fitur\n",
    "    ngram_range=(1, 3), # Gunakan Unigram, Bigram, dan Trigram\n",
    "    min_df=3 # Coba turunkan minimal dokumen yang mengandung term\n",
    "    # Optional: coba parameter lain seperti sublinear_tf=True, smooth_idf=True\n",
    "    # sublinear_tf=True,\n",
    "    # smooth_idf=True\n",
    ") # min_df=5: abaikan term yang muncul di kurang dari 5 dokumen\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text']) # Transform data test menggunakan vectorizer yang sudah di-fit\n",
    "\n",
    "print(\"Shape Data Training setelah TF-IDF:\", X_train_tfidf.shape)\n",
    "print(\"Shape Data Test setelah TF-IDF:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcc80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Langkah 4 & 5: Pemilihan & Pelatihan Model, Validasi & Tuning ---\n",
    "print(\"\\nLangkah 4 & 5: Pelatihan Model, Validasi & Tuning Threshold...\")\n",
    "\n",
    "# Strategy: Binary Relevance (satu model biner per label)\n",
    "# Model dasar: Logistic Regression\n",
    "# Gunakan solver 'liblinear' yang baik untuk data sparse\n",
    "base_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=RANDOM_STATE)\n",
    "multioutput_classifier = MultiOutputClassifier(base_model, n_jobs=-1) # n_jobs=-1: gunakan semua core CPU\n",
    "\n",
    "# Split data training untuk validasi\n",
    "X_train_part, X_val, y_train_part, y_val = train_test_split(\n",
    "    X_train_tfidf, y_train_bin, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    # Untuk stratifikasi multi-label, butuh library tambahan atau implementasi custom.\n",
    "    # Untuk demo ini, kita gunakan split standar dulu.\n",
    ")\n",
    "\n",
    "print(f\"Shape Data Training Part ({1-TEST_SIZE*100}%):\", X_train_part.shape)\n",
    "print(f\"Shape Data Validation ({TEST_SIZE*100}%):\", X_val.shape)\n",
    "\n",
    "print(\"Memulai Pelatihan Model Binary Relevance...\")\n",
    "multioutput_classifier.fit(X_train_part, y_train_part)\n",
    "print(\"Pelatihan Selesai.\")\n",
    "\n",
    "# Evaluasi pada Validation Set (Menggunakan Macro F1)\n",
    "# Model Logistic Regression dengan MultiOutputClassifier akan memberikan probabilitas\n",
    "y_val_proba = np.transpose([model.predict_proba(X_val)[:, 1] for model in multioutput_classifier.estimators_])\n",
    "\n",
    "\n",
    "# Tuning Threshold\n",
    "# Cari threshold terbaik untuk memaksimalkan Macro F1 di validation set\n",
    "best_threshold = 0.05 # Default\n",
    "best_macro_f1 = 0\n",
    "\n",
    "print(\"Mulai Tuning Threshold...\")\n",
    "# Coba range threshold dari 0.1 hingga 0.9 dengan langkah 0.05\n",
    "thresholds = np.arange(0.01, 1.0, 0.0001) #0.0001\n",
    "for threshold in thresholds:\n",
    "    # Konversi probabilitas menjadi prediksi biner menggunakan threshold saat ini\n",
    "    y_val_pred_tuned = (y_val_proba > threshold).astype(int)\n",
    "\n",
    "    # Hitung Macro F1 untuk threshold ini\n",
    "    macro_f1 = f1_score(y_val, y_val_pred_tuned, average='macro')\n",
    "\n",
    "    # Update threshold terbaik jika Macro F1 lebih baik\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Threshold Optimal yang ditemukan di Validation: {best_threshold:.4f}\")\n",
    "print(f\"Macro F1 di Validation dengan Threshold Optimal: {best_macro_f1:.4f}\")\n",
    "\n",
    "# Dengan threshold optimal, buat prediksi biner untuk validation set\n",
    "y_val_pred_optimal = (y_val_proba > best_threshold).astype(int)\n",
    "print(f\"Macro F1 di Validation (konfirmasi): {f1_score(y_val, y_val_pred_optimal, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------BARUUUU------------------------------------\n",
    "# Cell 5: Per-Label Threshold Tuning\n",
    "\n",
    "print(\"Cell 5: Per-Label Threshold Tuning...\")\n",
    "\n",
    "# Gunakan probabilitas prediksi dari model LGBM di validation set (dari Cell 4)\n",
    "# Variabel y_val_proba dan y_val seharusnya sudah tersedia dari Cell 4\n",
    "# Variabel thresholds (range threshold) juga harus sudah tersedia dari Cell 4\n",
    "# Variabel mlb (MultiLabelBinarizer) juga harus sudah tersedia dari Cell 2\n",
    "# Variabel label_classes (urutan nama label) juga harus sudah tersedia dari Cell 2\n",
    "\n",
    "current_y_val_proba = y_val_proba # Probabilitas prediksi di validation set dari model di Cell 4\n",
    "current_y_val = y_val # Ground truth biner validation set dari Cell 4\n",
    "\n",
    "# Simpan threshold optimal untuk setiap label\n",
    "best_thresholds_per_label = []\n",
    "macro_f1_per_label_individual_list = [] # Opsional, simpan F1 individual\n",
    "\n",
    "print(f\"Mulai Per-Label Threshold Tuning (Mencoba {len(thresholds)} threshold per label)...\")\n",
    "# Pastikan label_classes (urutan nama label) sudah tersedia dari Cell 2\n",
    "label_classes = list(mlb.classes_) # Ambil lagi untuk memastikan\n",
    "\n",
    "# Loop melalui setiap label (kolom di y_val_proba)\n",
    "for label_idx in range(current_y_val_proba.shape[1]):\n",
    "    # print(f\"  Tuning threshold untuk label: {label_classes[label_idx]}...\") # Opsional, untuk melihat progress\n",
    "    best_f1_for_label = -1 # F1 bisa 0, mulai dari -1\n",
    "    optimal_threshold_for_label = 0.5 # Default awal\n",
    "\n",
    "    # Dapatkan probabilitas dan target ground truth hanya untuk label ini\n",
    "    label_proba = current_y_val_proba[:, label_idx]\n",
    "    label_true = current_y_val[:, label_idx]\n",
    "\n",
    "    # Coba berbagai threshold dari range 'thresholds' yang didefinisikan di Cell 4\n",
    "    for threshold in thresholds:\n",
    "        label_pred_tuned = (label_proba > threshold).astype(int)\n",
    "\n",
    "        # Hitung F1 score untuk label ini\n",
    "        # zero_division=1 penting jika label_true tidak memiliki positive sample di validation\n",
    "        # (misal, label yang sangat jarang tidak muncul di split validation)\n",
    "        f1_for_label = f1_score(label_true, label_pred_tuned, zero_division=1)\n",
    "\n",
    "        # Update threshold terbaik untuk label ini\n",
    "        if f1_for_label > best_f1_for_label:\n",
    "            best_f1_for_label = f1_for_label\n",
    "            optimal_threshold_for_label = threshold\n",
    "\n",
    "    # Simpan threshold optimal dan F1 score terbaik untuk label ini\n",
    "    best_thresholds_per_label.append(optimal_threshold_for_label)\n",
    "    macro_f1_per_label_individual_list.append(best_f1_for_label)\n",
    "\n",
    "\n",
    "# Setelah tuning untuk semua label, hitung Macro F1 total dengan threshold optimal per label\n",
    "print(\"\\nMenghitung Macro F1 Total dengan Threshold Optimal Per-Label di Validation...\")\n",
    "# Terapkan array threshold ke array probabilitas\n",
    "y_val_pred_optimal_per_label = (current_y_val_proba > np.array(best_thresholds_per_label)).astype(int) # FIX: Gunakan current_y_val_proba\n",
    "macro_f1_optimal_per_label = f1_score(current_y_val, y_val_pred_optimal_per_label, average='macro', zero_division=1)\n",
    "\n",
    "print(f\"\\n--- Hasil Setelah Per-Label Threshold Tuning ---\")\n",
    "print(f\"Threshold Optimal Per-Label:\")\n",
    "for i, thresh in enumerate(best_thresholds_per_label):\n",
    "     print(f\"  {label_classes[i]}: {thresh:.4f}\")\n",
    "\n",
    "print(f\"\\nMacro F1 di Validation dengan Threshold Optimal Per-Label: {macro_f1_optimal_per_label:.4f}\")\n",
    "# Bandingkan dengan hasil global threshold LGBM dari Cell 4\n",
    "# Anda perlu mengingat skor global threshold dari output Cell 4, atau menyimpannya ke variabel di Cell 4\n",
    "# print(\"Perbandingan dengan Hasil Sebelumnya (Global Threshold):\", macro_f1_optimal_per_label - best_macro_f1_global)\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "# Variabel best_thresholds_per_label ini SANGAT PENTING. Ini akan digunakan untuk prediksi akhir pada data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Langkah 6: Prediksi dan Submission ---\n",
    "print(\"\\nLangkah 6: Prediksi pada Test Data dan Pembuatan Submission File...\")\n",
    "\n",
    "# Dapatkan probabilitas prediksi pada data test penuh\n",
    "y_test_proba = np.transpose([model.predict_proba(X_test_tfidf)[:, 1] for model in multioutput_classifier.estimators_])\n",
    "\n",
    "# Terapkan threshold optimal yang ditemukan dari validation set\n",
    "# y_test_pred_bin = (y_test_proba > best_threshold).astype(int)\n",
    "y_test_pred_bin = (y_test_proba > np.array(best_thresholds_tuned)).astype(int)\n",
    "\n",
    "# Konversi prediksi biner kembali ke format string label\n",
    "# Gunakan mlb.inverse_transform untuk mengubah array biner menjadi daftar label string\n",
    "test_pred_labels = mlb.inverse_transform(y_test_pred_bin)\n",
    "\n",
    "# Format untuk submission: gabungkan label per baris menjadi satu string\n",
    "submission_labels = [\" \".join(labels) for labels in test_pred_labels]\n",
    "\n",
    "# Buat DataFrame Submission\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'labels': submission_labels})\n",
    "\n",
    "# Simpan Submission File\n",
    "submission_df.to_csv('submission_lgbm_per_label.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' berhasil dibuat.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3cd84",
   "metadata": {},
   "source": [
    "# scibert 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Untuk text cleaning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Untuk multi-label encoding\n",
    "from sklearn.multioutput import MultiOutputClassifier # Untuk Binary Relevance strategy\n",
    "from sklearn.linear_model import LogisticRegression # Model klasifikasi biner\n",
    "from sklearn.metrics import f1_score # Metrik evaluasi\n",
    "import joblib # Untuk menyimpan model dan vectorizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# --- Tambahkan impor dan kode ini di sini ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import lightgbm as lgb # Akan digunakan nanti, import saja di awal\n",
    "\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2 # Ukuran validation set\n",
    "MAX_FEATURES = 10000 # Batasan jumlah fitur TF-IDF\n",
    "NGRAM_RANGE = (1, 2) # Menggunakan unigram dan bigram\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "# --- Tambahkan konfigurasi Transformer di sini ---\n",
    "MODEL_NAME = 'allenai/scibert_scivocab_uncased' # Model pre-trained SciBERT\n",
    "MAX_LEN = 256 # Panjang maksimum sequence token. Abstract bisa panjang. Sesuaikan ini.\n",
    "              # Meningkatkan MAX_LEN butuh lebih banyak VRAM/GPU. 256 adalah titik awal yang wajar.\n",
    "              # Pertimbangkan truncation=True saat t    okenizing jika teks lebih panjang.\n",
    "BATCH_SIZE = 16 # Ukuran batch untuk DataLoader. Sesuaikan dengan VRAM GPU Anda.\n",
    "                # Ukuran lebih besar bisa mempercepat pelatihan, tapi butuh lebih banyak VRAM.\n",
    "EPOCHS = 4 # Jumlah epoch pelatihan. Mulai dari nilai kecil.\n",
    "LEARNING_RATE = 2e-5 # Learning rate untuk fine-tuning Transformer. Nilai kecil standar (misal 1e-5, 2e-5, 3e-5).\n",
    "\n",
    "print(f\"Menggunakan model: {MODEL_NAME}\")\n",
    "# --- Tambahkan definisi range threshold juga di sini (jika belum ada atau mau disesuaikan) ---\n",
    "# Ini range threshold untuk tuning per-label nanti\n",
    "# thresholds_to_try = np.arange(0.01, 1.0, 0.0001) # Dari Cell 4 Anda\n",
    "# Jika mau lebih fine-grained di area probabilitas rendah:\n",
    "thresholds_to_try = np.concatenate([np.arange(0.001, 0.01, 0.001), np.arange(0.01, 1.0, 0.01)]) # Lebih banyak di range rendah\n",
    "print(f\"Range Thresholds untuk Tuning Per-Label: {thresholds_to_try[0]:.4f} - {thresholds_to_try[-1]:.4f} (Jumlah: {len(thresholds_to_try)})\")\n",
    "\n",
    "# --- Langkah 1: Data Loading dan Initial Explorasi ---\n",
    "print(\"Langkah 1: Loading Data dan Eksplorasi Awal...\")\n",
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "\n",
    "print(\"Data Training Shape:\", train_df.shape)\n",
    "print(\"Data Test Shape:\", test_df.shape)\n",
    "\n",
    "print(\"\\nInfo Data Training:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nMissing Values di Data Training:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values di Data Test:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Analisis Target Variabel (labels)\n",
    "# Mendapatkan daftar semua unique labels yang mungkin\n",
    "all_labels = sorted(list(set([lbl for labels in train_df['labels'] for lbl in labels.split()])))\n",
    "print(f\"\\nSemua Label SDG yang Teridentifikasi ({len(all_labels)}):\")\n",
    "print(all_labels)\n",
    "\n",
    "# Hitung frekuensi setiap label (untuk memahami imbalance)\n",
    "label_counts = {}\n",
    "for labels in train_df['labels']:\n",
    "    for label in labels.split():\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(\"\\nFrekuensi Setiap Label SDG:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "# Catatan: Macro F1 sangat dipengaruhi oleh performa pada label minor.\n",
    "# Frekuensi rendah menunjukkan label tersebut akan sulit diprediksi.\n",
    "\n",
    "# Pastikan device (GPU/CPU) terdeteksi\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Menggunakan device:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Langkah 2: Preprocessing Data ---\n",
    "print(\"\\nLangkah 2: Preprocessing Data...\")\n",
    "\n",
    "# Gabungkan title dan abstract\n",
    "train_df['text'] = train_df['title'].fillna('') + ' ' + train_df['abstract'].fillna('')\n",
    "test_df['text'] = test_df['title'].fillna('') + ' ' + test_df['abstract'].fillna('')\n",
    "\n",
    "# Fungsi Text Cleaning\n",
    "def clean_text(text): # Gunakan nama fungsi yang sudah ada di notebook Anda\n",
    "    text = str(text).lower() # Pastikan input adalah string\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Hapus non-alfabet dan non-spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Hapus spasi ganda dan trim\n",
    "\n",
    "    # --- Tambahkan baris ini untuk menghapus stop words ---\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOP_WORDS]\n",
    "    text = \" \".join(words)\n",
    "    # --- Akhir penambahan ---\n",
    "\n",
    "    return text\n",
    "\n",
    "print(\"Menerapkan Text Cleaning...\")\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Processing Label (Multi-label Binarization)\n",
    "print(\"Menerapkan Multi-label Binarization...\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Fit Binarizer pada semua label yang ada di data train\n",
    "y_train_bin = mlb.fit_transform(train_df['labels'].apply(lambda x: x.split()))\n",
    "\n",
    "# --- Tambahkan baris ini di sini: ---Baru-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "label_classes = list(mlb.classes_) # Menyimpan urutan nama label\n",
    "\n",
    "# --- Tambahkan kode ini di sini ------------------------------------------------------------------------\n",
    "print(\"\\nMenghitung Bobot Label untuk Weighted BCE Loss...\")\n",
    "# Total jumlah sampel\n",
    "total_samples = len(train_df)\n",
    "# Jumlah kemunculan SETIAP label secara total di SELURUH dataset\n",
    "# Ini BUKAN jumlah dokumen, ini jumlah total '1' di y_train_bin\n",
    "label_occurrences = np.sum(y_train_bin, axis=0)\n",
    "\n",
    "# Hitung bobot. Rumus umum: N_total / (N_kelas * N_kelas_total). Atau N_total / N_kelas.\n",
    "# Coba rumus sederhana: total_samples / label_occurrences\n",
    "# Atau lebih umum: total_samples / (jumlah_label * label_occurrences)\n",
    "# Atau inverse frequency: 1 / label_occurrences, lalu normalisasi\n",
    "# Rumus yang umum dan sering berhasil untuk multi-label:\n",
    "# Pos_weight[i] = (Total Negatif untuk Label i) / (Total Positif untuk Label i)\n",
    "# Total Negatif untuk Label i = total_samples - label_occurrences[i]\n",
    "\n",
    "label_weights = (total_samples - label_occurrences) / label_occurrences\n",
    "# PENTING: Urutan bobot harus sama dengan urutan label di mlb.classes_\n",
    "# label_classes = list(mlb.classes_) # mlb.classes_ sudah punya urutan yang benar\n",
    "# Pastikan label_occurrences juga dalam urutan yang benar, np.sum(y_train_bin, axis=0) sudah benar urutannya.\n",
    "\n",
    "# Konversi bobot ke tensor PyTorch dan pindahkan ke device\n",
    "# Bobot harus float\n",
    "label_weights_tensor = torch.tensor(label_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Bobot Label (dalam urutan label_classes):\")\n",
    "for i, label in enumerate(label_classes):\n",
    "    print(f\"  {label}: {label_weights[i]:.2f}\")\n",
    "\n",
    "print(f\"\\nBobot Label Tensor shape: {label_weights_tensor.shape}\")\n",
    "print(f\"Bobot Label Tensor device: {label_weights_tensor.device}\")\n",
    "# --- Akhir penambahan -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Label order setelah binarisasi\n",
    "print(\"Urutan Label setelah Binarization:\", list(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140cd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Transformer Data Preparation - Tokenization, Datasets, DataLoaders\n",
    "\n",
    "print(\"Cell 3: Transformer Data Preparation...\")\n",
    "\n",
    "# Gunakan kolom 'cleaned_text' dari Cell 2\n",
    "# Gunakan y_train_bin dari Cell 2\n",
    "# Gunakan RANDOM_STATE dan TEST_SIZE dari Cell 1\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "# Tokenizer ini yang akan mengubah teks menjadi ID numerik sesuai vocabulary model.\n",
    "# Ini menggantikan TfidfVectorizer\n",
    "print(f\"Memuat tokenizer: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"Tokenizer dimuat.\")\n",
    "\n",
    "# --- Tokenisasi Data ---\n",
    "print(f\"Menerapkan tokenisasi ke data training dan test (max_len={MAX_LEN})...\")\n",
    "\n",
    "# Fungsi untuk melakukan tokenisasi dan mengembalikan PyTorch tensors\n",
    "def tokenize_and_tensorize(text):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,    # Tambahkan token [CLS] dan [SEP]\n",
    "        max_length=MAX_LEN,         # Padding/Truncation ke panjang maksimum\n",
    "        padding='max_length',       # Pad ke max_length\n",
    "        return_attention_mask=True, # Buat attention mask\n",
    "        return_tensors='pt',        # Kembalikan sebagai PyTorch tensors\n",
    "        truncation=True             # Lakukan truncation jika lebih panjang dari max_len\n",
    "    )\n",
    "    # Squeeze untuk menghilangkan dimensi batch=1 dari output encode_plus\n",
    "    return {\n",
    "        'input_ids': encoded['input_ids'].squeeze(),\n",
    "        'attention_mask': encoded['attention_mask'].squeeze()\n",
    "    }\n",
    "\n",
    "# Tokenisasi semua teks. Hasilnya list of dictionaries.\n",
    "print(\"Tokenizing train text...\")\n",
    "train_encoded = [tokenize_and_tensorize(text) for text in train_df['cleaned_text']]\n",
    "print(\"Tokenizing test text...\")\n",
    "test_encoded = [tokenize_and_tensorize(text) for text in test_df['cleaned_text']]\n",
    "print(\"Tokenization selesai.\")\n",
    "\n",
    "\n",
    "# Pisahkan input_ids dan attention_mask ke dalam list terpisah\n",
    "train_input_ids = torch.stack([x['input_ids'] for x in train_encoded])\n",
    "train_attention_masks = torch.stack([x['attention_mask'] for x in train_encoded])\n",
    "\n",
    "test_input_ids = torch.stack([x['input_ids'] for x in test_encoded])\n",
    "test_attention_masks = torch.stack([x['attention_mask'] for x in test_encoded])\n",
    "\n",
    "print(f\"Shape train_input_ids: {train_input_ids.shape}\")\n",
    "print(f\"Shape train_attention_masks: {train_attention_masks.shape}\")\n",
    "print(f\"Shape test_input_ids: {test_input_ids.shape}\")\n",
    "print(f\"Shape test_attention_masks: {test_attention_masks.shape}\")\n",
    "\n",
    "# --- Data Biner Label (Sudah ada dari Cell 2, y_train_bin) ---\n",
    "# Konversi y_train_bin ke PyTorch Tensor (tipe float untuk loss)\n",
    "y_train_bin_tensor = torch.tensor(y_train_bin, dtype=torch.float)\n",
    "print(f\"Shape y_train_bin_tensor: {y_train_bin_tensor.shape}\")\n",
    "\n",
    "\n",
    "# --- Split Data Training untuk Validasi (Menggunakan Tensors) ---\n",
    "# Ini menggantikan split di Cell 4 sebelumnya\n",
    "print(f\"\\nMelakukan split data training ({1-TEST_SIZE*100}% train, {TEST_SIZE*100}% val)...\")\n",
    "train_indices, val_indices = train_test_split(\n",
    "    np.arange(len(train_df)), # Split indeksnya\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    # Multi-label stratification is complex, skip for now or use specialized lib\n",
    "    # stratify=y_train_bin # Simple stratification might not work well for multi-label\n",
    ")\n",
    "\n",
    "# Ambil tensor berdasarkan indeks split\n",
    "X_train_input_ids = train_input_ids[train_indices]\n",
    "X_train_attention_masks = train_attention_masks[train_indices]\n",
    "y_train_split = y_train_bin_tensor[train_indices]\n",
    "\n",
    "X_val_input_ids = train_input_ids[val_indices]\n",
    "X_val_attention_masks = train_attention_masks[val_indices]\n",
    "y_val_split = y_train_bin_tensor[val_indices]\n",
    "\n",
    "print(f\"Shape train_part input_ids: {X_train_input_ids.shape}\")\n",
    "print(f\"Shape val input_ids: {X_val_input_ids.shape}\")\n",
    "\n",
    "\n",
    "# --- Membuat PyTorch Datasets dan DataLoaders ---\n",
    "print(\"\\nMembuat PyTorch Datasets dan DataLoaders...\")\n",
    "\n",
    "# Dataset untuk train_part, validation, dan test\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_input_ids, X_train_attention_masks, y_train_split)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_input_ids, X_val_attention_masks, y_val_split)\n",
    "# Untuk data test, kita hanya butuh input dan attention mask, targetnya tidak ada\n",
    "test_dataset = torch.utils.data.TensorDataset(test_input_ids, test_attention_masks)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False) # Jangan shuffle validation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # Jangan shuffle test\n",
    "\n",
    "print(\"Datasets dan DataLoaders selesai dibuat.\")\n",
    "\n",
    "print(\"\\nData Preparation untuk Transformer Selesai.\")\n",
    "print(\"Langkah selanjutnya adalah membuat arsitektur model Transformer dan training loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4abcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Transformer Model Definition, Optimizer, Loss\n",
    "\n",
    "print(\"Cell 4: Transformer Model Definition, Optimizer, Loss...\")\n",
    "\n",
    "# Gunakan MODEL_NAME dari Cell 1\n",
    "# Gunakan device dari Cell 1\n",
    "# Gunakan jumlah label dari y_train_bin atau label_classes (Cell 2)\n",
    "NUM_LABELS = len(label_classes) # Jumlah label SDG (16)\n",
    "\n",
    "# --- Definisikan Arsitektur Model Transformer Multi-label ---\n",
    "# Kita akan menggunakan AutoModel (base Transformer tanpa head klasifikasi)\n",
    "# dan menambahkan layer linear (Dense) di atasnya untuk multi-label klasifikasi\n",
    "class TransformerMultiLabelClassifier(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TransformerMultiLabelClassifier, self).__init__()\n",
    "        # Muat model Transformer pre-trained\n",
    "        # Setting output_hidden_states=True bisa berguna, tapi default False sudah cukup\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        # Tambahkan layer klasifikasi linear di atas output [CLS] token\n",
    "        # Ukuran input layer linear adalah dimensi embedding dari model Transformer\n",
    "        # Misalnya, untuk SciBERT base uncased, hidden size-nya 768\n",
    "        self.classifier = torch.nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "        # Fungsi aktivasi Sigmoid untuk output (probabilitas independen per label)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Masukkan input ke model Transformer\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # Ambil output [CLS] token. Ini biasanya output pertama di sequence_output.\n",
    "        # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        cls_token_output = sequence_output[:, 0, :] # Ambil vektor untuk token [CLS] (indeks 0)\n",
    "\n",
    "        # Masukkan output [CLS] ke layer klasifikasi\n",
    "        logits = self.classifier(cls_token_output)\n",
    "        # Apply Sigmoid untuk mendapatkan probabilitas\n",
    "        probabilities = self.sigmoid(logits)\n",
    "\n",
    "        return probabilities, logits # Kembalikan probabilitas (untuk prediksi) dan logits (untuk loss)\n",
    "\n",
    "# --- Inisialisasi Model dan Pindah ke Device (GPU/CPU) ---\n",
    "print(\"Menginisialisasi model Transformer...\")\n",
    "model = TransformerMultiLabelClassifier(MODEL_NAME, NUM_LABELS)\n",
    "model.to(device) # Pindahkan model ke GPU (atau CPU)\n",
    "print(\"Model diinisialisasi.\")\n",
    "\n",
    "# --- Definisikan Optimizer dan Loss Function ---\n",
    "# Optimizer: AdamW seringkali standar untuk fine-tuning Transformer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss Function: Binary Cross-Entropy with Logits (BCELoss) untuk multi-label classification\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------baru\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=label_weights_tensor) # Gunakan bobot label\n",
    "print(\"Optimizer dan Weighted BCEWithLogitsLoss Function diinisialisasi.\")\n",
    "\n",
    "print(\"Optimizer dan Loss Function diinisialisasi.\")\n",
    "\n",
    "print(\"\\nModel Setup Selesai.\")\n",
    "print(\"Langkah selanjutnya adalah menulis training loop.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Transformer Training Loop\n",
    "\n",
    "print(\"Cell 5: Transformer Training Loop...\")\n",
    "\n",
    "# Gunakan model, optimizer, criterion, device dari Cell 4\n",
    "# Gunakan train_dataloader dan val_dataloader dari Cell 3\n",
    "# Gunakan EPOCHS dari Cell 1\n",
    "\n",
    "# --- Training Function (Optional, tapi baik untuk modularitas) ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train() # Set model ke mode training\n",
    "    total_loss = 0\n",
    "    # Loop melalui batch data training\n",
    "    for batch in dataloader:\n",
    "        # Pindahkan batch data ke device (GPU/CPU)\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device) # Target biner label\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # Kita butuh logits untuk BCELoss\n",
    "        probabilities, logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Hitung loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass dan optimasi\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() # Kumpulkan loss per batch\n",
    "\n",
    "    return total_loss / len(dataloader) # Rata-rata loss per epoch\n",
    "\n",
    "\n",
    "# --- Evaluation Function (Optional) ---\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval() # Set model ke mode evaluation\n",
    "    total_loss = 0\n",
    "    all_probabilities = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Nonaktifkan perhitungan gradient saat evaluasi\n",
    "    with torch.no_grad():\n",
    "        # Loop melalui batch data validation\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            probabilities, logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Hitung loss (optional, untuk monitor)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Kumpulkan probabilitas dan ground truth untuk evaluasi F1 nanti\n",
    "            all_probabilities.append(probabilities.cpu().numpy()) # Pindahkan ke CPU untuk numpy\n",
    "            all_labels.append(labels.cpu().numpy()) # Pindahkan ke CPU untuk numpy\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_probabilities = np.concatenate(all_probabilities)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Kembalikan probabilitas dan label untuk threshold tuning\n",
    "    return avg_loss, all_probabilities, all_labels\n",
    "\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "print(f\"\\nMemulai Training Loop ({EPOCHS} epochs)...\")\n",
    "\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", # Jenis scheduler\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0, # Mulai tanpa warmup (bisa diatur > 0)\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal langkah pelatihan: {total_steps}\")\n",
    "print(\"Learning Rate Scheduler diinisialisasi.\")\n",
    "\n",
    "\n",
    "best_val_macro_f1 = -1 # Lacak Macro F1 terbaik di validation\n",
    "best_val_thresholds = None # Simpan thresholds terbaik\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    # Latih\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Evaluasi\n",
    "    val_loss, val_probabilities, val_labels = evaluate_epoch(model, val_dataloader, criterion, device)\n",
    "    print(f\"  Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # --- Lakukan Per-Label Threshold Tuning di Validation Set setelah setiap epoch ---\n",
    "    # Ini seperti kode dari Cell 5 sebelumnya, tapi menggunakan probabilitas dari model Transformer\n",
    "    current_y_val_proba = val_probabilities # Probabilitas dari evaluate_epoch\n",
    "    current_y_val = val_labels # Ground truth dari evaluate_epoch\n",
    "\n",
    "    best_thresholds_current_epoch = []\n",
    "    macro_f1_per_label_individual_current_epoch = []\n",
    "\n",
    "    # Gunakan thresholds_to_try dari Cell 1\n",
    "    # Gunakan label_classes dari Cell 2\n",
    "\n",
    "    for label_idx in range(current_y_val_proba.shape[1]):\n",
    "        best_f1_for_label = -1\n",
    "        optimal_threshold_for_label = 0.5\n",
    "\n",
    "        label_proba = current_y_val_proba[:, label_idx]\n",
    "        label_true = current_y_val[:, label_idx]\n",
    "\n",
    "        for threshold in thresholds_to_try: # Gunakan range threshold yang didefinisikan di Cell 1\n",
    "            label_pred_tuned = (label_proba > threshold).astype(int)\n",
    "            f1_for_label = f1_score(label_true, label_pred_tuned, zero_division=1)\n",
    "\n",
    "            if f1_for_label > best_f1_for_label:\n",
    "                best_f1_for_label = f1_for_label\n",
    "                optimal_threshold_for_label = threshold\n",
    "\n",
    "        best_thresholds_current_epoch.append(optimal_threshold_for_label)\n",
    "        macro_f1_per_label_individual_current_epoch.append(best_f1_for_label)\n",
    "\n",
    "\n",
    "    # Hitung Macro F1 Total untuk epoch ini\n",
    "    y_val_pred_optimal_current_epoch = (current_y_val_proba > np.array(best_thresholds_current_epoch)).astype(int)\n",
    "    macro_f1_current_epoch = f1_score(current_y_val, y_val_pred_optimal_current_epoch, average='macro', zero_division=1)\n",
    "\n",
    "    print(f\"  Validation Macro F1 (dengan Threshold Optimal Per-Label): {macro_f1_current_epoch:.4f}\")\n",
    "\n",
    "    # Simpan model dan thresholds jika ini adalah epoch terbaik\n",
    "    if macro_f1_current_epoch > best_val_macro_f1:\n",
    "        best_val_macro_f1 = macro_f1_current_epoch\n",
    "        best_val_thresholds = best_thresholds_current_epoch.copy() # Simpan copy-nya\n",
    "        print(\"  ---> Macro F1 di Validation Meningkat. Menyimpan thresholds terbaik.\")\n",
    "        # Anda bisa tambahkan menyimpan model state_dict di sini jika mau\n",
    "        # torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "\n",
    "print(\"\\nTraining Loop Selesai.\")\n",
    "print(f\"Macro F1 terbaik di Validation: {best_val_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c13229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Transformer Prediction on Test and Submission\n",
    "\n",
    "print(\"Cell 6: Transformer Prediction and Submission...\")\n",
    "\n",
    "# Gunakan model (yang sudah dilatih di Cell 5)\n",
    "# Gunakan test_dataloader dari Cell 3\n",
    "# Gunakan device dari Cell 1\n",
    "# Gunakan best_val_thresholds dari Cell 5\n",
    "# Gunakan mlb dari Cell 2\n",
    "# Gunakan test_df['id'] dari Cell 1\n",
    "\n",
    "# --- Prediksi Probabilitas pada Test Data ---\n",
    "print(\"Membuat prediksi probabilitas pada data test menggunakan model Transformer...\")\n",
    "\n",
    "model.eval() # Set model ke mode evaluation\n",
    "test_probabilities = []\n",
    "\n",
    "with torch.no_grad(): # Nonaktifkan perhitungan gradient\n",
    "    for batch in test_dataloader:\n",
    "        # Data test tidak punya label, hanya input_ids dan attention_mask\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        probabilities, logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Kumpulkan probabilitas\n",
    "        test_probabilities.append(probabilities.cpu().numpy())\n",
    "\n",
    "# Gabungkan semua probabilitas dari batch\n",
    "test_probabilities = np.concatenate(test_probabilities)\n",
    "\n",
    "print(\"Prediksi probabilitas pada data test selesai.\")\n",
    "print(f\"Shape test_probabilities: {test_probabilities.shape}\")\n",
    "\n",
    "\n",
    "# --- Terapkan Threshold Optimal Per-Label ---\n",
    "# Gunakan best_val_thresholds yang ditemukan di Cell 5\n",
    "# Pastikan best_val_thresholds tersedia di memori\n",
    "\n",
    "print(\"\\nMenerapkan threshold optimal per-label untuk konversi biner...\")\n",
    "# best_val_thresholds adalah list, konversi ke numpy array untuk perbandingan\n",
    "test_pred_bin_final = (test_probabilities > np.array(best_val_thresholds)).astype(int)\n",
    "print(\"Penerapan threshold selesai.\")\n",
    "\n",
    "\n",
    "# --- Konversi Prediksi Biner ke Format String Label ---\n",
    "# Gunakan mlb (MultiLabelBinarizer) dari Cell 2\n",
    "print(\"\\nMengkonversi prediksi biner ke string label...\")\n",
    "test_pred_labels_final = mlb.inverse_transform(test_pred_bin_final) # mlb dari Cell 2\n",
    "\n",
    "# Format untuk submission\n",
    "submission_labels_final = [\" \".join(labels) for labels in test_pred_labels_final]\n",
    "print(\"Konversi ke string label selesai.\")\n",
    "\n",
    "\n",
    "# --- Buat DataFrame Submission dan Simpan File ---\n",
    "print(\"\\nMembuat DataFrame submission...\")\n",
    "submission_df_final = pd.DataFrame({'id': test_df['id'], 'labels': submission_labels_final}) # test_df dari Cell 1\n",
    "\n",
    "# Simpan Submission File\n",
    "print(\"\\nMenyimpan submission file 'submission.csv'...\")\n",
    "submission_df_final.to_csv('submission_scibert.csv', index=False)\n",
    "\n",
    "print(\"\\n--- Submission file 'submission.csv' berhasil dibuat ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e797a1c",
   "metadata": {},
   "source": [
    "# join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a824de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load kedua fn_filled = pd.ren_filled.csv')\n",
    "submission_log = pd.read_csv('submission_lgbm_per_label.csv')\n",
    "\n",
    "\n",
    "# Cek dulu kolom kuncinya, misalnya pakai 'id' atau 'filename'\n",
    "# Misal kita pakai 'id' sebagai kunci\n",
    "key_column = 'id'  # ganti sesuai kolom unik yang dipakai di kedua file\n",
    "\n",
    "# Merge dua DataFrame berdasarkan key\n",
    "mn_filled.merge(submission_log[[key_column, 'labels']], on=key_column, how='left', suffixes=('', '_log'))\n",
    "\n",
    "# Isi nilai yang kosong di 'labels' dengan 'labels_log' dari submission_lgbm.csv\n",
    "merged_final['labels'] = merged['labels'].fillna(merged['labels_log'])\n",
    "\n",
    "# Hapus kolom bantu\n",
    "merged = merged.drop(columns=['labels_log'])\n",
    "\n",
    "# Simpan hasilnya\n",
    "merged.to_csv('submission_filled.csv', index=False)\n",
    "\n",
    "print(\"Done! Hasil disimpan di submission_filled.csv\")\n",
    "\n",
    "# Merged 2\n",
    "submission_filled = pd.read_csv(\"submission_filled.csv\")\n",
    "submission_log = pd.read_csv('submission_log.csv')\n",
    "\n",
    "key_column = 'id'  # ganti sesuai kolom unik yang dipakai di kedua file\n",
    "\n",
    "# Merge dua DataFrame berdasarkan key\n",
    "merged_final = submission_filled.merge(submission_log[[key_column, 'labels']], on=key_column, how='left', suffixes=('', '_log'))\n",
    "\n",
    "# Isi nilai yang kosong di 'labels' dengan 'labels_log' dari submission_log.csv\n",
    "merged_final['labels'] = merged_final['labels'].fillna(merged_final['labels_log'])\n",
    "\n",
    "# Hapus kolom bantu\n",
    "merged_final = merged_final.drop(columns=['labels_log'])\n",
    "\n",
    "# Simpan hasilnya\n",
    "merged_final.to_csv('submission_final.csv', index=False)\n",
    "\n",
    "print(\"Done! Hasil disimpan di submission_filled.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
